{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import shutil\n",
    "import pickle\n",
    "from collections import deque\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from data_generation.data_generator import DataGenerator\n",
    "from util.misc import save_checkpoint\n",
    "from util.load_config_files import load_yaml_into_dotdict\n",
    "from util.plotting import output_truth_plot, compute_avg_certainty, get_constrastive_ax, get_false_ax, get_total_loss_ax\n",
    "from util.logger import Logger\n",
    "from modules.loss import MotLoss, FalseMeasurementLoss\n",
    "from modules.contrastive_loss import ContrastiveLoss\n",
    "from modules import evaluator\n",
    "from modules.models.mt3.mt3 import MOTT\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\t# Load CLI arguments\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument('--task_params', default='/home/weixinwei/study/MT3-test/configs/tasks/task1.yaml')\n",
    "\tparser.add_argument('--model_params', default='/home/weixinwei/study/MT3-test/configs/models/mt3.yaml')\n",
    "\tparser.add_argument('--continue_training_from', default=None)\n",
    "\tparser.add_argument('--exp_name', default=None)\n",
    "\t# args = parser.parse_args()\n",
    "\targs = parser.parse_known_args()[0]\n",
    "\t# args={\"continue_training_from\": None, \"exp_name\": None, \n",
    "\t# \t\t\"model_params\": '/home/weixinwei/study/MT3-test/configs/models/mt3.yaml', \n",
    "\t# \t\t\"task_params\": '/home/weixinwei/study/MT3-test/configs/tasks/task1.yaml'}\n",
    "\tprint(f'Task configuration file: {args.task_params}')\n",
    "\tprint(f'Model configuration file: {args.model_params}')\n",
    "\n",
    "\t# Load hyperparameters\n",
    "\tparams = load_yaml_into_dotdict(args.task_params)\n",
    "\tparams.update(load_yaml_into_dotdict(args.model_params))\n",
    "\n",
    "\tif params.training.device == 'auto':\n",
    "\t\tparams.training.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\t# Create logger and save all code dependencies imported so far\n",
    "\tcur_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "\tresults_folder_path = cur_path + os.sep + 'results'\n",
    "\texp_name = args.exp_name if args.exp_name is not None else time.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\t\n",
    "\tlogger = Logger(log_path=f'{results_folder_path}/{exp_name}', save_output=False)\n",
    "\tprint(f\"Saving results to folder {logger.log_path}\")\n",
    "\tlogger.save_code_dependencies(project_root_path=os.path.realpath('../'))  # assuming this is ran from repo root\n",
    "\n",
    "\t# Manually copy the configuration yaml file used for this experiment to the logger folder\n",
    "\tshutil.copy(args.task_params, os.path.join(logger.log_path, 'code_used', 'task_params.yaml'))\n",
    "\tshutil.copy(args.model_params, os.path.join(logger.log_path, 'code_used', 'model_params.yaml'))\n",
    "\n",
    "\t# If continuing an experiment, manually copy the `code_used` of the experiment from which training wil continue\n",
    "\tif args.continue_training_from is not None:\n",
    "\t\ttry:\n",
    "\t\t\tshutil.copytree(os.path.join(args.continue_training_from, 'code_used'),\n",
    "\t\t\t\t\t\t\tos.path.join(logger.log_path, 'code_from_previous_training'))\n",
    "\t\texcept FileNotFoundError:\n",
    "\t\t\tprint(f'Path specified to continue training from does not exist: {args.continue_training_from}')\n",
    "\t\t\texit()\n",
    "\n",
    "\tmodel = MOTT(params)\n",
    "\tdata_generator = DataGenerator(params)\n",
    "\tmot_loss = MotLoss(params)\n",
    "\tcontrastive_loss = ContrastiveLoss(params)\n",
    "\tfalse_loss = FalseMeasurementLoss(params)\n",
    "\n",
    "\t# Optionally load the model weights from a provided checkpoint\n",
    "\tif args.continue_training_from is not None:\n",
    "\t\t# Find filename for last checkpoint available\n",
    "\t\tcheckpoints_path = os.path.join(args.continue_training_from, 'checkpoints')\n",
    "\t\tcheckpoint_names = os.listdir(checkpoints_path)\n",
    "\t\tidx_last = np.argmax([int(re.findall(r\"\\d+\", c)[-1]) for c in checkpoint_names])  # extract last occurrence of a number from the names\n",
    "\t\tlast_filename = os.path.join(checkpoints_path, checkpoint_names[idx_last])\n",
    "\n",
    "\t\t# Load model weights and pass model to correct device\n",
    "\t\tcheckpoint = torch.load(last_filename)\n",
    "\t\tmodel.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\tmodel.to(torch.device(params.training.device))\n",
    "\toptimizer = Adam(model.parameters(), lr=params.training.learning_rate)\n",
    "\tscheduler = ReduceLROnPlateau(optimizer,\n",
    "\t\t\t\t\t\t\t\t  patience=params.training.reduce_lr_patience,\n",
    "\t\t\t\t\t\t\t\t  factor=params.training.reduce_lr_factor,\n",
    "\t\t\t\t\t\t\t\t  verbose=params.debug.print_reduce_lr_messages)\n",
    "\t# Optionally load optimizer and scheduler states from provided checkpoint (this has to be done after loading the\n",
    "\t# model weights and calling model.to(), to guarantee these will be in the correct device too)\n",
    "\tif args.continue_training_from is not None:\n",
    "\t\toptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\t\tscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "\tcurrent_lr = optimizer.param_groups[0]['lr']\n",
    "\tlogger.log_scalar('metrics/learning_rate', current_lr, 0)\n",
    "\n",
    "\tif params.debug.enable_plot or params.debug.save_plot_figs:\n",
    "\t\tfig = plt.figure(constrained_layout=True, figsize=(15, 8))\n",
    "\t\tfig.canvas.set_window_title('Training Progress')\n",
    "\n",
    "\t\tgs = GridSpec(2, 3, figure=fig)\n",
    "\t\tloss_ax = fig.add_subplot(gs[0, 0])\n",
    "\t\tloss_ax.set_ylabel('Loss', color='C0')\n",
    "\t\tloss_ax.grid('on')\n",
    "\t\tloss_line, = loss_ax.plot([1], 'r', label='Loss', c='C0')\n",
    "\t\tloss_ax.tick_params(axis='y', labelcolor='C0')\n",
    "\t\tloss_ax.set_yscale('log')\n",
    "\n",
    "\t\tpercent_ax = fig.add_subplot(gs[1, 0])\n",
    "\t\tpercent_ax.set_ylabel('Certainty distribution')\n",
    "\t\tpercent_ax.grid('on')\n",
    "\t\tmatched_median_cert_line, = percent_ax.plot([1], 'C0', label='Matched median certainty')\n",
    "\t\tunmatched_median_cert_line, = percent_ax.plot([1], 'C3', label='Unmatched median certainty')\n",
    "\t\tmax_cert_line, = percent_ax.plot([1], 'C0--', label='Max certainty')\n",
    "\t\tmin_cert_line, = percent_ax.plot([1], 'C0--', label='Min certainty')\n",
    "\t\t\n",
    "\t\toutput_ax = fig.add_subplot(gs[:, 1:])\n",
    "\t\toutput_ax.set_ylabel('Y')\n",
    "\t\toutput_ax.set_xlabel('X')\n",
    "\n",
    "\t\tif params.debug.save_plot_figs:\n",
    "\t\t\tos.makedirs(os.path.join(logger.log_path, 'figs', 'main'))\n",
    "\t\t\ttotal_loss_fig, total_loss_ax, total_loss_line = get_total_loss_ax()\n",
    "\t\t\tos.makedirs(os.path.join(logger.log_path, 'figs', 'aux'))\n",
    "\n",
    "\t\tif params.loss.contrastive_classifier:\n",
    "\t\t\tcontrastive_loss_fig, contrastive_loss_ax, contrastive_loss_line = get_constrastive_ax()\n",
    "\t\t\tos.makedirs(os.path.join(logger.log_path, 'figs', 'aux', 'contrastive'))\n",
    "\t\t\t\n",
    "\t\tif params.loss.false_classifier:\n",
    "\t\t\tfalse_loss_fig, false_loss_ax, false_loss_line = get_false_ax()\n",
    "\t\t\tos.makedirs(os.path.join(logger.log_path, 'figs', 'aux', 'false'))\n",
    "\n",
    "\tlosses = []\n",
    "\tlast_layer_losses = []\n",
    "\tc_losses = []\n",
    "\tf_losses = []\n",
    "\tmatched_min_certainties = []\n",
    "\tmatched_q1_certainties = []\n",
    "\tmatched_median_certainties = []\n",
    "\tmatched_q3_certainties = []\n",
    "\tmatched_max_certainties = []\n",
    "\tunmatched_min_certainties = []\n",
    "\tunmatched_q1_certainties = []\n",
    "\tunmatched_median_certainties = []\n",
    "\tunmatched_q3_certainties = []\n",
    "\tunmatched_max_certainties = []\n",
    "\n",
    "\toutputs_history = deque(maxlen=50)\n",
    "\tindices_history = deque(maxlen=50)\n",
    "\n",
    "\tprint(\"[INFO] Training started...\")\n",
    "\tstart_time = time.time()\n",
    "\ttime_since = time.time()\n",
    "\n",
    "\tfor i_gradient_step in range(params.training.n_gradient_steps):\n",
    "\t\ttry:\n",
    "\t\t\tbatch, labels, unique_ids, trajectories = data_generator.get_batch()\n",
    "\t\t\toutputs, memory, aux_classifications, queries, attn_maps  = model.forward(batch, unique_ids)\n",
    "\t\t\tloss_dict, indices = mot_loss.forward(outputs, labels, loss_type=params.loss.type)\n",
    "\n",
    "\t\t\tif params.loss.type == 'both':\n",
    "\t\t\t\tgospa_weight = (i_gradient_step / params.training.n_gradient_steps)**3\n",
    "\t\t\t\ttotal_loss = sum(gospa_weight * loss_dict[k] if ('gospa' in k) else loss_dict[k] for k in loss_dict.keys())\n",
    "\t\t\t\tlogger.log_scalar(f'metrics/detr', loss_dict['detr'], i_gradient_step)\n",
    "\t\t\t\tlogger.log_scalar(f'metrics/gospa', loss_dict['gospa'], i_gradient_step)\n",
    "\t\t\t\tlast_layer_losses.append(loss_dict['detr'].item() + loss_dict['gospa'].item()*gospa_weight)\n",
    "\t\t\telse:\n",
    "\t\t\t\ttotal_loss = sum(loss_dict[k] for k in loss_dict.keys())\n",
    "\t\t\t\tlast_layer_losses.append(loss_dict[params.loss.type].item())\n",
    "\t\t\t\tlogger.log_scalar(f'metrics/{params.loss.type}', loss_dict[params.loss.type], i_gradient_step)\n",
    "\n",
    "\t\t\tif params.loss.contrastive_classifier:\n",
    "\t\t\t\tc_loss = contrastive_loss(aux_classifications['contrastive_classifications'], unique_ids)\n",
    "\t\t\t\ttotal_loss = total_loss + c_loss * params.loss.c_loss_multiplier\n",
    "\t\t\t\tc_losses.append(c_loss.item())\n",
    "\t\t\t\tlogger.log_scalar('metrics/contrastive_loss', c_loss, i_gradient_step)\n",
    "\n",
    "\t\t\tif params.loss.false_classifier:\n",
    "\t\t\t\tf_loss = false_loss(aux_classifications['false_classifications'], unique_ids)\n",
    "\t\t\t\ttotal_loss = total_loss + f_loss * params.loss.f_loss_multiplier\n",
    "\t\t\t\tf_losses.append(f_loss.item())\n",
    "\t\t\t\tlogger.log_scalar('metrics/false_loss', f_loss, i_gradient_step)\n",
    "\n",
    "\t\t\tlosses.append(total_loss.item())\n",
    "\t\t\tlogger.log_scalar('metrics/total_loss', total_loss.item(), i_gradient_step)\n",
    "\t\t\t\n",
    "\t\t\tif params.loss.return_intermediate:\n",
    "\t\t\t\tfor k, v in loss_dict.items():\n",
    "\t\t\t\t\tif '_' in k:\n",
    "\t\t\t\t\t\tlogger.log_scalar('metrics/'+k, v.item(), i_gradient_step)\n",
    "\n",
    "\t\t\t# Compute quantiles for matched and unmatched predictions\n",
    "\t\t\toutputs_history.append({'state': outputs['state'].detach().cpu(), 'logits': outputs['logits'].detach().cpu()})\n",
    "\t\t\tindices_history.append(indices)\n",
    "\t\t\tmatched_quants, unmatched_quants = compute_avg_certainty(outputs_history, indices_history)\n",
    "\t\t\tmin_cert, q1_cert, median_cert, q3_cert, max_cert = matched_quants\n",
    "\t\t\tmatched_min_certainties.append(min_cert)\n",
    "\t\t\tmatched_q1_certainties.append(q1_cert)\n",
    "\t\t\tmatched_median_certainties.append(median_cert)\n",
    "\t\t\tmatched_q3_certainties.append(q3_cert)\n",
    "\t\t\tmatched_max_certainties.append(max_cert)\n",
    "\t\t\tlogger.log_scalar('metrics/matched_min_certainty', min_cert, i_gradient_step)\n",
    "\t\t\tlogger.log_scalar('metrics/matched_q1_certainty', q1_cert, i_gradient_step)\n",
    "\t\t\tlogger.log_scalar('metrics/matched_median_certainty', median_cert, i_gradient_step)\n",
    "\t\t\tlogger.log_scalar('metrics/matched_q3_certainty', q3_cert, i_gradient_step)\n",
    "\t\t\tlogger.log_scalar('metrics/matched_max_certainty', max_cert, i_gradient_step)\n",
    "\n",
    "\t\t\tmin_cert, q1_cert, median_cert, q3_cert, max_cert = unmatched_quants\n",
    "\t\t\tunmatched_min_certainties.append(min_cert)\n",
    "\t\t\tunmatched_q1_certainties.append(q1_cert)\n",
    "\t\t\tunmatched_median_certainties.append(median_cert)\n",
    "\t\t\tunmatched_q3_certainties.append(q3_cert)\n",
    "\t\t\tunmatched_max_certainties.append(max_cert)\n",
    "\t\t\tlogger.log_scalar('metrics/unmatched_min_certainty', min_cert, i_gradient_step)\n",
    "\t\t\tlogger.log_scalar('metrics/unmatched_q1_certainty', q1_cert, i_gradient_step)\n",
    "\t\t\tlogger.log_scalar('metrics/unmatched_median_certainty', median_cert, i_gradient_step)\n",
    "\t\t\tlogger.log_scalar('metrics/unmatched_q3_certainty', q3_cert, i_gradient_step)\n",
    "\t\t\tlogger.log_scalar('metrics/unmatched_max_certainty', max_cert, i_gradient_step)\n",
    "\t\t\t\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\ttotal_loss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# Update learning rate, logging it if changed\n",
    "\t\t\tscheduler.step(total_loss)\n",
    "\t\t\tnew_lr = optimizer.param_groups[0]['lr']\n",
    "\t\t\tif new_lr != current_lr:\n",
    "\t\t\t\tcurrent_lr = new_lr\n",
    "\t\t\t\tlogger.log_scalar('metrics/learning_rate', current_lr, i_gradient_step)\n",
    "\n",
    "\t\t\tif i_gradient_step % params.debug.print_interval == 0:\n",
    "\t\t\t\tcur_time = time.time()\n",
    "\t\t\t\tt = str(datetime.timedelta(seconds=round(cur_time - time_since)))\n",
    "\t\t\t\tt_tot = str(datetime.timedelta(seconds=round(cur_time - start_time)))\n",
    "\t\t\t\tprint(f\"Number of gradient steps: {i_gradient_step + 1} \\t \"\n",
    "\t\t\t\t\t  f\"Loss: {np.mean(losses[-15:])} \\t \"\n",
    "\t\t\t\t\t  f\"Time per step: {(cur_time-time_since)/params.debug.print_interval} \\t \"\n",
    "\t\t\t\t\t  f\"Total time elapsed: {t_tot}\")\n",
    "\t\t\t\ttime_since = time.time()\n",
    "\n",
    "\t\t\tif (params.debug.enable_plot and i_gradient_step % params.debug.plot_interval == 0) or \\\n",
    "\t\t\t\t\t(params.debug.save_plot_figs and i_gradient_step % params.debug.save_plot_figs_interval == 0):\n",
    "\t\t\t\tx_axis = list(range(i_gradient_step+1))\n",
    "\t\t\t\tloss_line.set_data(x_axis, last_layer_losses)\n",
    "\t\t\t\tloss_ax.relim()\n",
    "\t\t\t\tloss_ax.autoscale_view()\n",
    "\n",
    "\t\t\t\tpercent_ax.collections.clear()\n",
    "\t\t\t\tmatched_median_cert_line.set_data(x_axis, np.array(matched_median_certainties))\n",
    "\t\t\t\tpercent_ax.fill_between(x_axis, matched_min_certainties, matched_max_certainties, color='C0', alpha=0.3, linewidth=0.0)\n",
    "\t\t\t\tpercent_ax.fill_between(x_axis, matched_q1_certainties, matched_q3_certainties, color='C0', alpha=0.6, linewidth=0.0)\n",
    "\t\t\t\tunmatched_median_cert_line.set_data(x_axis, np.array(unmatched_median_certainties))\n",
    "\t\t\t\tpercent_ax.fill_between(x_axis, unmatched_min_certainties, unmatched_max_certainties, color='C3', alpha=0.3, linewidth=0.0)\n",
    "\t\t\t\tpercent_ax.fill_between(x_axis, unmatched_q1_certainties, unmatched_q3_certainties, color='C3', alpha=0.6, linewidth=0.0)\n",
    "\t\t\t\tpercent_ax.set_ylim([-0.05, 1.05])\n",
    "\n",
    "\t\t\t\toutput_ax.cla()\n",
    "\t\t\t\toutput_ax.grid('on')\n",
    "\t\t\t\toutput_truth_plot(output_ax, outputs, labels, indices, batch)\n",
    "\t\t\t\toutput_ax.set_xlim([params.data_generation.field_of_view_lb, params.data_generation.field_of_view_ub])\n",
    "\t\t\t\toutput_ax.set_ylim([params.data_generation.field_of_view_lb, params.data_generation.field_of_view_ub])\n",
    "\n",
    "\t\t\t\tif params.loss.contrastive_classifier:\n",
    "\t\t\t\t\tcontrastive_loss_line.set_data(x_axis, c_losses)\n",
    "\t\t\t\t\tcontrastive_loss_ax.relim()\n",
    "\t\t\t\t\tcontrastive_loss_ax.autoscale_view()\n",
    "\n",
    "\t\t\t\tif params.loss.false_classifier:\n",
    "\t\t\t\t\tfalse_loss_line.set_data(x_axis, f_losses)\n",
    "\t\t\t\t\tfalse_loss_ax.relim()\n",
    "\t\t\t\t\tfalse_loss_ax.autoscale_view()\n",
    "\n",
    "\t\t\t\tif (params.debug.enable_plot and i_gradient_step % params.debug.plot_interval == 0):\n",
    "\t\t\t\t\tfig.canvas.draw()\n",
    "\t\t\t\t\tplt.pause(0.01)\n",
    "\n",
    "\t\t\t\tif params.debug.save_plot_figs and i_gradient_step % params.debug.save_plot_figs_interval == 0:\n",
    "\t\t\t\t\tfilename = f\"gradient_step{i_gradient_step}.jpg\"\n",
    "\t\t\t\t\tfig.savefig(os.path.join(logger.log_path, 'figs', 'main', filename))\n",
    "\n",
    "\t\t\t\t\ttotal_loss_line.set_data(x_axis, losses)\n",
    "\t\t\t\t\ttotal_loss_ax.relim()\n",
    "\t\t\t\t\ttotal_loss_ax.autoscale_view()\n",
    "\t\t\t\t\ttotal_loss_fig.savefig(os.path.join(logger.log_path, 'figs', 'aux', filename))\n",
    "\n",
    "\t\t\t\t\tif params.loss.contrastive_classifier:\n",
    "\t\t\t\t\t\tcontrastive_loss_fig.savefig(os.path.join(logger.log_path, 'figs', 'aux', 'contrastive', filename))\n",
    "\t\t\t\t\tif params.loss.false_classifier:\n",
    "\t\t\t\t\t\tfalse_loss_fig.savefig(os.path.join(logger.log_path, 'figs', 'aux', 'false', filename))\n",
    "\n",
    "\t\texcept KeyboardInterrupt:\n",
    "\t\t\tfilename = f'checkpoint_gradient_step_{i_gradient_step}'\n",
    "\t\t\tfolder_name = os.path.join(logger.log_path, 'checkpoints')\n",
    "\t\t\tsave_checkpoint(folder=folder_name,\n",
    "\t\t\t\t\t\t\tfilename=filename,\n",
    "\t\t\t\t\t\t\tmodel=model,\n",
    "\t\t\t\t\t\t\toptimizer=optimizer,\n",
    "\t\t\t\t\t\t\tscheduler=scheduler)\n",
    "\t\t\tprint(\"[INFO] Exiting...\")\n",
    "\t\t\tdata_generator.pool.close()\n",
    "\t\t\texit()\n",
    "\n",
    "\t\t# Save checkpoint\n",
    "\t\tif (i_gradient_step+1) % params.training.checkpoint_interval == 0:\n",
    "\t\t\tfilename = f'checkpoint_gradient_step_{i_gradient_step}'\n",
    "\t\t\tfolder_name = os.path.join(logger.log_path, 'checkpoints')\n",
    "\t\t\tsave_checkpoint(folder=folder_name,\n",
    "\t\t\t\t\t\t\tfilename=filename,\n",
    "\t\t\t\t\t\t\tmodel=model,\n",
    "\t\t\t\t\t\t\toptimizer=optimizer,\n",
    "\t\t\t\t\t\t\tscheduler=scheduler)\n",
    "\n",
    "\t# Evaluate afterwards\n",
    "\n",
    "\t# Setting seed for reproducibility\n",
    "\tparams.data_generation.seed = 0\n",
    "\t# Only 1 training example per batch\n",
    "\tparams.training.batch_size = 1\n",
    "\t# Reset data-generator for reproducibility\n",
    "\tdata_generator = DataGenerator(params)\n",
    "\n",
    "\tog, pg, d = evaluator.evaluate_metrics(data_generator, model, params, mot_loss,  num_eval=1000, verbose=True)\n",
    "\tprint(\"Finished running evaluation... please paste this in the spread-sheet\")\n",
    "\tprint(f\"{np.mean(og['output'])} \\t {np.var(og['output'])} \\t {np.mean(pg['output'])} \\t {np.var(pg['output'])} \\t {np.mean(d['output'])} \\t {np.var(d['output'])}\")\n",
    "\tprint(\".\"*150)\n",
    "\tos.makedirs(os.path.join(logger.log_path, 'eval'), exist_ok=True)\n",
    "\tpickle.dump(og, open(os.path.join(logger.log_path, 'eval', 'original_gospa.p'), \"wb\"))\n",
    "\tpickle.dump(pg, open(os.path.join(logger.log_path, 'eval', 'prob_gospa.p'), \"wb\"))\n",
    "\tpickle.dump(d, open(os.path.join(logger.log_path, 'eval', 'detr.p'), \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('wxw_MT3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "347784c50ede1b8272d3277b31bd1efc47294a65da090260761e3995fd8a0fe0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
