{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import argparse\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import SeedSequence, default_rng\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "from util.load_config_files import load_yaml_into_dotdict\n",
    "from util.misc import NestedTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task configuration file: /home/weixinwei/study/MT3-test/configs/tasks/task1.yaml\n",
      "Model configuration file: /home/weixinwei/study/MT3-test/configs/models/mt3.yaml\n"
     ]
    }
   ],
   "source": [
    "# 从CLI载入yaml\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--task_params', default='/home/weixinwei/study/MT3-test/configs/tasks/task1.yaml')\n",
    "parser.add_argument('--model_params', default='/home/weixinwei/study/MT3-test/configs/models/mt3.yaml')\n",
    "args = parser.parse_known_args()[0]\n",
    "print(f'Task configuration file: {args.task_params}')\n",
    "print(f'Model configuration file: {args.model_params}')\n",
    "\n",
    "# 从yaml载入超参数\n",
    "params = load_yaml_into_dotdict(args.task_params)\n",
    "params.update(load_yaml_into_dotdict(args.model_params))\n",
    "\n",
    "# 生成训练数据\n",
    "data_generator = DataGenerator(params)\n",
    "# batch, labels, unique_ids, trajectories = data_generator.get_batch()\n",
    "# training_data, labels, unique_ids, trajectories, new_rngs = data_generator.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, labels, unique_ids, trajectories, new_rngs = data_generator.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10.11082918,  36.12177396],\n",
       "       [ 25.96212182,  -5.65003554]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_data[6]\n",
    "# unique_ids[6]\n",
    "labels[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据绘制\n",
    "colorEnum = ['r', 'y', 'g', 'c', 'b', 'm', 'r', 'y', 'g', 'c', 'b', 'm']\n",
    "batchRange = [2]\n",
    "for measBatch in batchRange:\n",
    "\tmeasPosX = batch.tensors[measBatch, ~batch.mask[measBatch], 0]\n",
    "\tmeasPosY = batch.tensors[measBatch, ~batch.mask[measBatch], 1]\n",
    "\tmeasAlph = batch.tensors[measBatch, ~batch.mask[measBatch], 2]/max(batch.tensors[measBatch, ~batch.mask[measBatch], 2])\n",
    "\tplt.scatter(measPosX, measPosY, color='k', s=20, marker='.', alpha=measAlph)\n",
    "\tplt.scatter(labels[measBatch].T[0], labels[measBatch].T[1], color='b', marker='x', alpha=1)\n",
    "\t# for targetNo in list(trajectories[measBatch]):\n",
    "\t# \ttrackPosX = trajectories[measBatch][targetNo].T[0]\n",
    "\t# \ttrackPosY = trajectories[measBatch][targetNo].T[1]\n",
    "\t# \ttrackAlph = trajectories[measBatch][targetNo].T[4]/max(trajectories[measBatch][targetNo].T[4])\n",
    "\t# \tplt.scatter(trackPosX, trackPosY, color=colorEnum[targetNo], marker='x', alpha=trackAlph)\n",
    "\n",
    "\tplt.xlim((-10, 10))\n",
    "\tplt.ylim((-10, 10))\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object:\n",
    "\n",
    "\tdef __init__(self, pos, vel, t, delta_t, sigma, id):\n",
    "\t\tself.pos = pos\n",
    "\t\tself.vel = vel\n",
    "\t\tself.delta_t = delta_t\n",
    "\t\tself.sigma = sigma\n",
    "\t\tself.state_history = np.array([np.concatenate([pos,vel,np.array([t])])])\n",
    "\t\tself.process_noise_matrix = sigma*np.array([[delta_t ** 3 / 3, delta_t ** 2 / 2], [delta_t ** 2 / 2, delta_t]])\n",
    "\n",
    "\t\t# Unique identifier for every object\n",
    "\t\tself.id = id\n",
    "\n",
    "\tdef update(self, t, rng):\n",
    "\t\t\"\"\"\n",
    "\t\tUpdates this object's state using a discretized constant velocity model.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# Update position and velocity of the object in each dimension separately\n",
    "\t\tassert len(self.pos) == len(self.vel)\n",
    "\t\tprocess_noise = rng.multivariate_normal([0, 0], self.process_noise_matrix, size=len(self.pos))\n",
    "\t\tself.pos += self.delta_t * self.vel + process_noise[:,0]\n",
    "\t\tself.vel += process_noise[:,1]\n",
    "\n",
    "\t\t# Add current state to previous states\n",
    "\t\tself.state_history = np.vstack((self.state_history,np.concatenate([self.pos.copy(),self.vel.copy(),np.array([t])])))\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn 'id: {}, pos: {}, vel: {}'.format(self.id, self.pos, self.vel)\n",
    "\n",
    "class MotDataGenerator:\n",
    "\tdef __init__(self, args, rng):\n",
    "\t\tself.start_pos_params = [args.data_generation.mu_x0, args.data_generation.std_x0]\n",
    "\t\tself.start_vel_params = [args.data_generation.mu_v0, args.data_generation.std_v0]\n",
    "\t\tself.prob_add_obj = args.data_generation.p_add\n",
    "\t\tself.prob_remove_obj = args.data_generation.p_remove\n",
    "\t\tself.delta_t = args.data_generation.dt\n",
    "\t\tself.process_noise_intens = args.data_generation.sigma_q\n",
    "\t\tself.prob_measure = args.data_generation.p_meas\n",
    "\t\tself.measure_noise_intens = args.data_generation.sigma_y\n",
    "\t\tself.n_average_false_measurements = args.data_generation.n_avg_false_measurements\n",
    "\t\tself.n_average_starting_objects = args.data_generation.n_avg_starting_objects\n",
    "\t\tself.field_of_view_lb = args.data_generation.field_of_view_lb\n",
    "\t\tself.field_of_view_ub = args.data_generation.field_of_view_ub\n",
    "\t\tself.max_objects = args.data_generation.max_objects\n",
    "\t\tself.rng = rng\n",
    "\t\tself.dim = len(self.start_pos_params[0])\n",
    "\n",
    "\t\tself.debug = False\n",
    "\t\tassert self.n_average_starting_objects != 0, 'Datagen does not currently work with n_avg_starting_objects equal to zero.'\n",
    "\n",
    "\t\tself.t = None\n",
    "\t\tself.objects = None\n",
    "\t\tself.trajectories = None\n",
    "\t\tself.measurements = None\n",
    "\t\tself.unique_ids = None\n",
    "\t\tself.unique_id_counter = None\n",
    "\t\tself.reset()\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\tself.t = 0\n",
    "\t\tself.objects = []\n",
    "\t\tself.trajectories = {}\n",
    "\t\tself.measurements = np.array([])\n",
    "\t\tself.unique_ids = np.array([], dtype='int64')\n",
    "\t\tself.unique_id_counter = itertools.count()\n",
    "\n",
    "\t\t# Add initial set of objects (re-sample until we get a nonzero value)\n",
    "\t\tn_starting_objects = 0\n",
    "\t\twhile n_starting_objects == 0:\n",
    "\t\t\tn_starting_objects = self.rng.poisson(self.n_average_starting_objects)\n",
    "\t\tself.add_objects(n_starting_objects)\n",
    "\n",
    "\t\t# Measure the initial set of objects\n",
    "\t\tself.generate_measurements()\n",
    "\n",
    "\t\tif self.debug:\n",
    "\t\t\tprint(n_starting_objects, 'starting objects')\n",
    "\n",
    "\tdef create_new_object(self, pos, vel):\n",
    "\t\treturn Object(pos=pos,\n",
    "\t\t\t\t\t  vel=vel,\n",
    "\t\t\t\t\t  t=self.t,\n",
    "\t\t\t\t\t  delta_t=self.delta_t,\n",
    "\t\t\t\t\t  sigma=self.process_noise_intens,\n",
    "\t\t\t\t\t  id=next(self.unique_id_counter))\n",
    "\n",
    "\tdef add_objects(self, n):\n",
    "\t\t\"\"\"\n",
    "\t\tAdds `n` new objects to `objects` list.\n",
    "\t\t\"\"\"\n",
    "\t\t# Never add more objects than the maximum number of allowed objects\n",
    "\t\tn = min(n, self.max_objects-len(self.objects))\n",
    "\t\tif n == 0:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\t# Create new objects and save them in the datagen\n",
    "\t\tpositions = self.rng.uniform(low=self.field_of_view_lb, high=self.field_of_view_ub, size=(n,self.dim))\n",
    "\t\tvelocities = self.rng.multivariate_normal(self.start_vel_params[0], self.start_vel_params[1], size=(n,))\n",
    "\t\tself.objects += [self.create_new_object(pos, vel) for pos,vel in zip(positions, velocities)]\n",
    "\n",
    "\tdef remove_far_away_objects(self):\n",
    "\t\tif len(self.objects) == 0:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\tpositions = np.array([obj.pos for obj in self.objects])\n",
    "\t\tlb = positions < self.field_of_view_lb\n",
    "\t\tub = positions > self.field_of_view_ub\n",
    "\t\tremove_elements = np.bitwise_or(lb.any(axis=1), ub.any(axis=1))\n",
    "\n",
    "\t\tself.objects = [o for o, r in zip(self.objects, remove_elements) if not r]\n",
    "\n",
    "\tdef remove_objects(self, p):\n",
    "\t\t\"\"\"\n",
    "\t\tRemoves each of the objects with probability `p`.\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# Compute which objects are removed in this time-step\n",
    "\t\tdeaths = self.rng.binomial(n=1, p=p, size=len(self.objects))\n",
    "\n",
    "\t\tn_deaths = sum(deaths)\n",
    "\t\tif self.debug and (n_deaths > 0):\n",
    "\t\t\tprint(n_deaths, 'objects were removed')\n",
    "\n",
    "\t\t# Save the trajectories of the removed objects\n",
    "\t\tfor obj, death in zip(self.objects, deaths):\n",
    "\t\t\tif death:\n",
    "\t\t\t\tself.trajectories[obj.id] = obj.state_history\n",
    "\n",
    "\t\t# Remove them from the object list\n",
    "\t\tself.objects = [o for o, d in zip(self.objects, deaths) if not d]\n",
    "\n",
    "\tdef get_prob_death(self, obj):\n",
    "\t\treturn self.prob_remove_obj\n",
    "\n",
    "\tdef remove_object(self, obj, p = None):\n",
    "\t\t\"\"\"\n",
    "\t\tRemoves an object based on its state\n",
    "\t\t\"\"\"\n",
    "\t\tif p is None:\n",
    "\t\t\tp = self.get_prob_death(obj)\n",
    "\n",
    "\t\tr = self.rng.rand()\n",
    "\n",
    "\t\tif r < p:\n",
    "\t\t\treturn True\n",
    "\t\telse:\n",
    "\t\t\treturn False\n",
    "\n",
    "\tdef generate_measurements(self):\n",
    "\t\t\"\"\"\n",
    "\t\tGenerates all measurements (true and false) for the current time-step.\n",
    "\t\t\"\"\"\n",
    "\t\t# Generate the measurement for each object with probability `self.prob_measure`\n",
    "\t\tis_measured = self.rng.binomial(n=1, p=self.prob_measure, size=len(self.objects))\n",
    "\t\tmeasured_objects = [obj for obj, is_measured in zip(self.objects, is_measured) if is_measured]\n",
    "\t\tmeasurement_noise = self.rng.normal(0, self.measure_noise_intens, size=(len(measured_objects),self.dim))\n",
    "\t\ttrue_measurements = np.array([np.append(obj.pos+noise, self.t) for obj, noise in zip(measured_objects, measurement_noise)])\n",
    "\n",
    "\t\t# Generate false measurements\n",
    "\t\tn_false_measurements = self.rng.poisson(self.n_average_false_measurements)\n",
    "\t\tfalse_meas = self.rng.uniform(self.field_of_view_lb, self.field_of_view_ub, size=(n_false_measurements,self.dim))\n",
    "\t\tfalse_measurements = np.ones((n_false_measurements,self.dim+1)) * self.t\n",
    "\t\tfalse_measurements[:,:-1] = false_meas\n",
    "\n",
    "\t\t# Also save from which object each measurement came from (for contrastive learning later); -1 is for false meas.\n",
    "\t\tunique_obj_ids_true = [obj.id for obj in measured_objects]\n",
    "\t\tunique_obj_ids_false = [-1]*len(false_measurements)\n",
    "\t\tunique_obj_ids = np.array(unique_obj_ids_true + unique_obj_ids_false)\n",
    "\n",
    "\t\t# Concatenate true and false measurements in a single array\n",
    "\t\tif true_measurements.shape[0] and false_measurements.shape[0]:\n",
    "\t\t\tnew_measurements = np.vstack([true_measurements, false_measurements])\n",
    "\t\telif true_measurements.shape[0]:\n",
    "\t\t\tnew_measurements = true_measurements\n",
    "\t\telif false_measurements.shape[0]:\n",
    "\t\t\tnew_measurements = false_measurements\n",
    "\t\telse:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\t# Shuffle all generated measurements and corresponding unique ids in unison\n",
    "\t\trandom_idxs = self.rng.permutation(len(new_measurements))\n",
    "\t\tnew_measurements = new_measurements[random_idxs]\n",
    "\t\tunique_obj_ids = unique_obj_ids[random_idxs]\n",
    "\n",
    "\t\t# Save measurements and unique ids\n",
    "\t\tself.measurements = np.vstack([self.measurements, new_measurements]) if self.measurements.shape[0] else new_measurements\n",
    "\t\tself.unique_ids = np.hstack([self.unique_ids, unique_obj_ids])\n",
    "\n",
    "\tdef step(self, add_new_objects=True):\n",
    "\t\t\"\"\"\n",
    "\t\tPerforms one step of the simulation.\n",
    "\t\t\"\"\"\n",
    "\t\tself.t += self.delta_t\n",
    "\n",
    "\t\t# Update the remaining ones\n",
    "\t\tfor obj in self.objects:\n",
    "\t\t\tobj.update(self.t, self.rng)\n",
    "\n",
    "\t\t# Remove objects that left the field-of-view\n",
    "\t\tself.remove_far_away_objects()\n",
    "\n",
    "\t\t# Add new objects\n",
    "\t\tif add_new_objects:\n",
    "\t\t\tn_new_objs = self.rng.poisson(self.prob_add_obj)\n",
    "\t\t\tself.add_objects(n_new_objs)\n",
    "\n",
    "\t\t# Remove some of the objects\n",
    "\t\tself.remove_objects(self.prob_remove_obj)\n",
    "\t\t\n",
    "\t\t# Generate measurements\n",
    "\t\tself.generate_measurements()\n",
    "\t\t\n",
    "\t\tif self.debug:\n",
    "\t\t\tif n_new_objs > 0:\n",
    "\t\t\t\tprint(n_new_objs, 'objects were added')\n",
    "\t\t\tprint(len(self.objects))\n",
    "\n",
    "\tdef finish(self):\n",
    "\t\t\"\"\"\n",
    "\t\tShould be called after the last call to `self.step()`. Removes the remaining objects, consequently adding the\n",
    "\t\tremaining parts of their trajectories to `self.trajectories`.\n",
    "\t\t\"\"\"\n",
    "\t\tself.remove_objects(1.0)\n",
    "\n",
    "class DataGenerator:\n",
    "\tdef __init__(self, params):\n",
    "\t\tself.params = params\n",
    "\t\tassert 0 <= params.data_generation.n_prediction_lag <= params.data_generation.n_timesteps, \"Prediction lag has to be smaller than the total number of time-steps.\"\n",
    "\t\tself.device = params.training.device\n",
    "\t\tself.n_timesteps = params.data_generation.n_timesteps\n",
    "\n",
    "\t\tself.pool = multiprocessing.Pool()\n",
    "\n",
    "\t\t# Create `batch_size` data generators, each with its own independent (to a high probability) RNG\n",
    "\t\tss = SeedSequence(params.data_generation.seed)\n",
    "\t\trngs = [default_rng(s) for s in ss.spawn(params.training.batch_size)]\n",
    "\t\tself.datagens = [MotDataGenerator(params, rng=rng) for rng in rngs]\n",
    "\n",
    "\tdef get_batch(self):\n",
    "\t\tresults = self.pool.starmap(get_single_training_example, zip(self.datagens, [self.n_timesteps]*len(self.datagens)))\n",
    "\n",
    "\t\t# Unpack results\n",
    "\t\ttraining_data, labels, unique_ids, trajectories, new_rngs = tuple(zip(*results))\n",
    "\t\treturn training_data, labels, unique_ids, trajectories, new_rngs\n",
    "\t\tlabels = [Tensor(l).to(torch.device(self.device)) for l in labels]\n",
    "\t\ttrajectories = list(trajectories)\n",
    "\t\tunique_ids = [list(u) for u in unique_ids]\n",
    "\n",
    "\t\t# Update the RNGs of all the datagens for next call\n",
    "\t\tfor datagen, new_rng in zip(self.datagens, new_rngs):\n",
    "\t\t\tdatagen.rng = new_rng\n",
    "\n",
    "\t\t# Pad training data\n",
    "\t\tmax_len = max(list(map(len, training_data)))\n",
    "\t\ttraining_data, mask = pad_to_batch_max(training_data, max_len)\n",
    "\n",
    "\t\t# Pad unique ids\n",
    "\t\tfor i in range(len(unique_ids)):\n",
    "\t\t\tunique_id = unique_ids[i]\n",
    "\t\t\tn_items_to_add = max_len - len(unique_id)\n",
    "\t\t\tunique_ids[i] = np.concatenate([unique_id, [-2] * n_items_to_add])[None, :]\n",
    "\t\tunique_ids = np.concatenate(unique_ids)\n",
    "\n",
    "\t\ttraining_nested_tensor = NestedTensor(Tensor(training_data).to(torch.device(self.device)),\n",
    "\t\t\t\t\t\t\t\t\t\t\t  Tensor(mask).bool().to(torch.device(self.device)))\n",
    "\t\tunique_ids = Tensor(unique_ids).to(self.device)\n",
    "\n",
    "\t\treturn training_nested_tensor, labels, unique_ids, trajectories\n",
    "\n",
    "def pad_to_batch_max(training_data, max_len):\n",
    "\tbatch_size = len(training_data)\n",
    "\td_meas = training_data[0].shape[1]\n",
    "\ttraining_data_padded = np.zeros((batch_size, max_len, d_meas))\n",
    "\tmask = np.ones((batch_size, max_len))\n",
    "\tfor i, ex in enumerate(training_data):\n",
    "\t\ttraining_data_padded[i,:len(ex),:] = ex\n",
    "\t\tmask[i,:len(ex)] = 0\n",
    "\n",
    "\treturn training_data_padded, mask\n",
    "\n",
    "def get_single_training_example(data_generator, n_timesteps):\n",
    "\t\"\"\"Generates a single training example\n",
    "\n",
    "\tReturns:\n",
    "\t\ttraining_data   : A single training example\n",
    "\t\ttrue_data       : Ground truth for example\n",
    "\t\"\"\"\n",
    "\n",
    "\tdata_generator.reset()\n",
    "\tlabel_data = []\n",
    "\n",
    "\twhile len(label_data) == 0 or len(data_generator.measurements) == 0:\n",
    "\n",
    "\t\t# Generate n_timesteps of data, from scratch\n",
    "\t\tdata_generator.reset()\n",
    "\t\tfor i in range(n_timesteps - 1):\n",
    "\t\t\tadd_new_objects_flag = i < n_timesteps -3  # don't add new objects in the last two timesteps of generation, for cleaner training labels\n",
    "\t\t\tdata_generator.step(add_new_objects=add_new_objects_flag)\n",
    "\t\tdata_generator.finish()\n",
    "\n",
    "\t\t# -1 is applied because we count t=0 as one time-step\n",
    "\t\tfor traj_id in data_generator.trajectories:\n",
    "\t\t\ttraj = data_generator.trajectories[traj_id]\n",
    "\t\t\tif round(traj[-1][-1] / data_generator.delta_t) == n_timesteps - 1: #last state of trajectory, time\n",
    "\t\t\t\tpos = traj[-1][:data_generator.dim].copy()\n",
    "\t\t\t\tlabel_data.append(pos)\n",
    "\n",
    "\ttraining_data = np.array(data_generator.measurements.copy())\n",
    "\tunique_measurement_ids = data_generator.unique_ids.copy()\n",
    "\tnew_rng = data_generator.rng\n",
    "\n",
    "\treturn training_data, np.array(label_data), unique_measurement_ids, data_generator.trajectories.copy(), new_rng"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('wxw_MT3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "347784c50ede1b8272d3277b31bd1efc47294a65da090260761e3995fd8a0fe0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
